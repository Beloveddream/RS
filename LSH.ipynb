{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSH.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJd/ZVJVd7VAQfxl2lMMwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beloveddream/RS/blob/main/LSH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GHUIfVlcYUG",
        "outputId": "45ab823c-d00a-4b25-e448-e93c4f54c687"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryyWW6vSdw26",
        "outputId": "b48f8f74-0e04-47c9-b305-ef9ff4d3604a"
      },
      "source": [
        "pip install datasketch"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasketch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/35/3e39356d97dc67c4bddaddb51693c20a6eb61e535ce5be09d3755ba2b823/datasketch-1.5.3-py2.py3-none-any.whl (67kB)\n",
            "\r\u001b[K     |████▉                           | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 22.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 27.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 40kB 30.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 28.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from datasketch) (1.19.5)\n",
            "Installing collected packages: datasketch\n",
            "Successfully installed datasketch-1.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHBLg6TDdrpE",
        "outputId": "73c58f5a-0d81-4c83-ca17-ad50148d1571"
      },
      "source": [
        "from datasketch import MinHash\n",
        "data1 = ['这个', '程序', '代码', '太乱', '那个', '代码', '规范']\n",
        "data2 = ['这个', '程序', '代码', '不', '规范', '那个', '更', '规范']\n",
        "\n",
        "\n",
        "m1 = MinHash()\n",
        "m2 = MinHash()\n",
        "for d in data1:\n",
        "\tm1.update(d.encode('utf8'))\n",
        "for d in data2:\n",
        "    m2.update(d.encode('utf8'))\n",
        "print(\"使用MinHash预估的Jaccard相似度\", m1.jaccard(m2))\n",
        "\n",
        "s1 = set(data1)\n",
        "s2 = set(data2)\n",
        "actual_jaccard = float(len(s1.intersection(s2)))/float(len(s1.union(s2)))\n",
        "print(\"Jaccard相似度实际值\", actual_jaccard)\n",
        "print(s1.intersection(s2))\n",
        "print(len(s1.intersection(s2)))\n",
        "print(len(s1.union(s2)))\n",
        "print(s1.union(s2))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用MinHash预估的Jaccard相似度 0.6015625\n",
            "Jaccard相似度实际值 0.625\n",
            "{'代码', '程序', '规范', '那个', '这个'}\n",
            "5\n",
            "8\n",
            "{'太乱', '代码', '程序', '不', '规范', '更', '那个', '这个'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdwZJ_X4d9XK",
        "outputId": "7d60ae4e-76c9-410e-af7b-573d71188352"
      },
      "source": [
        "from datasketch import MinHash, MinHashLSH\n",
        "data1 = ['这个', '程序', '代码', '太乱', '那个', '代码', '规范']\n",
        "data2 = ['这个', '程序', '代码', '不', '规范', '那个', '更', '规范']\n",
        "data3 = ['这个', '程序', '代码', '不', '规范', '那个', '规范', '些']\n",
        "\n",
        "# 创建MinHash对象\n",
        "m1 = MinHash()\n",
        "m2 = MinHash()\n",
        "m3 = MinHash()\n",
        "for d in data1:\n",
        "\tm1.update(d.encode('utf8'))\n",
        "for d in data2:\n",
        "\tm2.update(d.encode('utf8'))\n",
        "for d in data3:\n",
        "\tm3.update(d.encode('utf8'))\n",
        "# 创建LSH\n",
        "lsh = MinHashLSH(threshold=0.6, num_perm=128)\n",
        "lsh.insert(\"m2\", m2)\n",
        "lsh.insert(\"m3\", m3)\n",
        "result = lsh.query(m2)\n",
        "print(\"近似的邻居（Jaccard相似度>0.5）\", result)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "近似的邻居（Jaccard相似度>0.5） ['m3', 'm2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tdPHgLhfXpa",
        "outputId": "0ab61efd-79fb-48bd-d52e-50aaba03ee03"
      },
      "source": [
        "pip install simhash"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting simhash\n",
            "  Downloading https://files.pythonhosted.org/packages/76/f8/65a885dd8615c6e241f6ff24e9479bba5bce2bd966aa4f71d772e3a65a19/simhash-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simhash) (1.19.5)\n",
            "Installing collected packages: simhash\n",
            "Successfully installed simhash-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovFliIT4e9-2",
        "outputId": "1688d21c-7737-4568-ea03-d473156bbe7c"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from simhash import Simhash, SimhashIndex\n",
        "\n",
        "print(Simhash('这个程序代码太乱').value)\n",
        "#print(Simhash('1').distance(Simhash('2')))\n",
        "#sh1 = Simhash(u'这个程序代码太乱,那个代码规范')\n",
        "#sh2 = Simhash(u'这个程序代码不规范,那个更规范')\n",
        "#print(sh1.value)\n",
        "#print(sh2.value)\n",
        "#print(sh1.distance(sh2))\n",
        "\n",
        "data = [\n",
        "    '这个程序代码太乱,那个代码规范',\n",
        "    '这个程序代码不规范,那个更规范',\n",
        "    '我是佩奇，这是我的弟弟乔治'\n",
        "]\n",
        "\n",
        "data = [\n",
        "    '这个 程序 代码 太乱 那个 代码 规范',\n",
        "    '这个 程序 代码 不 规范 那个 更 规范',\n",
        "    '我 是 佩奇 这 是 我的 弟弟 乔治'\n",
        "]\n",
        "\n",
        "vec = TfidfVectorizer()\n",
        "D = vec.fit_transform(data)\n",
        "voc = dict((i, w) for w, i in vec.vocabulary_.items())\n",
        "#print(voc)\n",
        "\n",
        "# 生成Simhash\n",
        "sh_list = []\n",
        "for i in range(D.shape[0]):\n",
        "    Di = D.getrow(i)\n",
        "    #print(Di.indices)\n",
        "    #print(Di.data)\n",
        "    # features表示 (token, weight)元祖形式的列表\n",
        "    features = zip([voc[j] for j in Di.indices], Di.data)\n",
        "    sh_list.append(Simhash(features))\n",
        "print(sh_list[0].distance(sh_list[1]))\n",
        "print(sh_list[0].distance(sh_list[2]))\n",
        "print(sh_list[1].distance(sh_list[2]))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10845600900754665595\n",
            "15\n",
            "32\n",
            "37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6owPxZsqgp89",
        "outputId": "d4241897-e4c8-451f-c7c7-b5a97d29f6d3"
      },
      "source": [
        "from datasketch import MinHash, MinHashLSH, MinHashLSHForest\n",
        "data1 = ['这个', '程序', '代码', '太乱', '那个', '代码', '规范']\n",
        "data2 = ['这个', '程序', '代码', '不', '规范', '那个', '更', '规范']\n",
        "data3 = ['这个', '程序', '代码', '不', '规范', '那个', '规范', '些']\n",
        "\n",
        "# 创建MinHash对象\n",
        "m1 = MinHash()\n",
        "m2 = MinHash()\n",
        "m3 = MinHash()\n",
        "for d in data1:\n",
        "\tm1.update(d.encode('utf8'))\n",
        "for d in data2:\n",
        "\tm2.update(d.encode('utf8'))\n",
        "for d in data3:\n",
        "\tm3.update(d.encode('utf8'))\n",
        "# 创建LSH Forest\n",
        "forest = MinHashLSHForest()\n",
        "forest.add(\"m2\", m2)\n",
        "forest.add(\"m3\", m3)\n",
        "# 在检索前，需要使用index\n",
        "forest.index()\n",
        "# 判断forest是否存在m2, m3\n",
        "print(\"m2\" in forest)\n",
        "print(\"m3\" in forest)\n",
        "# 查询forest中与m1相似的Top-K个邻居\n",
        "result = forest.query(m1, 2)\n",
        "print(\"Top 2 邻居\", result)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "Top 2 邻居 ['m3', 'm2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfVzBjOog5AS",
        "outputId": "c51987f3-05ec-4365-83d6-380e709c072f"
      },
      "source": [
        "from datasketch import MinHash, MinHashLSH, MinHashLSHEnsemble\n",
        "data1 = ['这个', '程序', '代码', '太乱', '那个', '代码', '规范']\n",
        "data2 = ['这个', '程序', '代码', '不', '规范', '那个', '更', '规范']\n",
        "data3 = ['这个', '程序', '代码', '不', '规范', '那个', '规范', '些']\n",
        "\n",
        "# 创建MinHash对象\n",
        "m1 = MinHash()\n",
        "m2 = MinHash()\n",
        "m3 = MinHash()\n",
        "for d in data1:\n",
        "\tm1.update(d.encode('utf8'))\n",
        "for d in data2:\n",
        "\tm2.update(d.encode('utf8'))\n",
        "for d in data3:\n",
        "\tm3.update(d.encode('utf8'))\n",
        "# 创建LSH Ensemble\n",
        "lshensemble = MinHashLSHEnsemble(threshold=0.8, num_perm=128)\n",
        "# Index takes an iterable of (key, minhash, size)\n",
        "lshensemble.index([(\"m2\", m2, len(data2)), (\"m3\", m3, len(data3))])\n",
        "# 判断lshensemble是否存在m2, m3\n",
        "print(\"m2\" in lshensemble)\n",
        "print(\"m3\" in lshensemble)\n",
        "# 查询与m1相似度大于0.8的集合\n",
        "print(\"与m1相似度大于0.8的集合：\")\n",
        "for key in lshensemble.query(m1, len(data1)):\n",
        "    print(key)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "与m1相似度大于0.8的集合：\n",
            "m3\n",
            "m2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUIAhEvthFcq"
      },
      "source": [
        "# 对天猫双11新闻进行相似句子Top-K查询\n",
        "from datasketch import MinHash, MinHashLSH, MinHashLSHForest\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba.posseg as pseg\n",
        "import re"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bifxHZgkhYJb"
      },
      "source": [
        "# 读取文件\n",
        "f = open('/content/gdrive/My Drive/LSH/sentences.txt', 'r', encoding='UTF-8')\n",
        "text = f.read()\n",
        "# 以句号，叹号，问号作为分隔，去掉\\n换行符号\n",
        "sentences = re.split('[。！？]', text.replace('\\n', ''))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c72Tlq5bhrZB",
        "outputId": "5add7d0a-ba89-4653-d6b1-d76ba34fea12"
      },
      "source": [
        "# 最后一行如果为空，则删除\n",
        "if sentences[len(sentences)-1] == '':\n",
        "    sentences.pop()\n",
        "print(sentences)\n",
        "print(len(sentences))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['11月10日深夜到11日凌晨，杭州阿里巴巴园区光柱擎天、灯火辉煌、不眠不休', '这是第11年双11，一个稀疏平常的日子凭空成为全民购物狂欢本就不可思议，而更不可思议的是不断刷新的消费数据', '仅用时96秒，2019天猫双11总成交额便超过100亿元，这个速度，比2018年缩短了29秒', '随后，500亿元、1000亿元，每一个成交额小目标都以更快的速度实现', '截至中证君发稿，2019天猫双11前三小时总成交额达到1325.7亿元，已经超过2018年天猫双11全天2135亿元总成交额的62%', '2018年，阿里巴巴用15小时49分钟39秒时间，打破了2017年1682亿元的双11全天成交纪录，最终以27%的增速定格在2135亿元', '今年双11，阿里巴巴又将以怎样的增速、多长时间来打破自己的纪录，它能否一举突破3000亿元大关，同时提振全国内需', '作为双11购物狂欢盛典的鼻祖，阿里巴巴在过去三年双11全网成交额中占据三分之二天下，京东、苏宁以及近期势头凶猛的拼多多均处于“后来”但尚未“居上”状态', '首先来看阿里巴巴（天猫）战报', '据介绍，今年，超过1000万款商品登陆天猫双11，超100万款新品在天猫双11首发，来自78个国家和地区的跨境电商进口商品、全国1000个数字农业基地的高品质农产品、2000个传统制造业产业带的工厂直供好货，以新供给满足消费者的多元化需求', '00:01:36，2019天猫双11总成交额超100亿元', '2018年用时2分05秒，今年缩短29秒', '00:12:49，2019天猫双11总成交额超500亿元', '2018年用时26分3秒，今年缩短一半', '01:03:59，2019天猫双11总成交额超1000亿元', '比2018年快了43分钟27秒，比2017年快了将近8小时']\n",
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yIJ7kiJhzb5",
        "outputId": "74177e84-c407-4b81-b979-e6c18a274098"
      },
      "source": [
        "# 将item_text进行分词\n",
        "def get_item_str(item_text):\n",
        "    item_str = \"\" \n",
        "    item=(pseg.cut(item_text)) \n",
        "    for i in list(item):\n",
        "        #去掉停用词\n",
        "        if i.word not in list(stop):  \n",
        "            item_str += i.word\n",
        "            #tfidf_vectorizer.fit_transform的输入需要空格分隔的单词\n",
        "            item_str += \" \"\n",
        "    return item_str\n",
        "# 对item_str创建MinHash\n",
        "def get_minhash(item_str):\n",
        "    temp = MinHash()\n",
        "    for d in item_str:\n",
        "        temp.update(d.encode('utf8'))\n",
        "    return temp\n",
        "\n",
        "# 设置停用词\n",
        "#stop = [line.strip().decode('utf-8') for line in open('stopword.txt').readlines()]\n",
        "stop = []\n",
        "# 得到分词后的documents\n",
        "documents = []\n",
        "for item_text in sentences:\n",
        "    # 将item_text进行分词\n",
        "    item_str = get_item_str(item_text)\n",
        "    documents.append(item_str)\n",
        "\n",
        "# 创建LSH Forest及MinHash对象\n",
        "minhash_list = []\n",
        "forest = MinHashLSHForest()\n",
        "for i in range(len(documents)):\n",
        "    #得到train_documents[i]的MinHash\n",
        "    temp = get_minhash(documents[i])\n",
        "    minhash_list.append(temp)\n",
        "    forest.add(i, temp)\n",
        "# index所有key，以便可以进行检索\n",
        "forest.index()\n",
        "\n",
        "query = '00:01:36，2019天猫双11总成交额超100亿元'\n",
        "# 将item_text进行分词\n",
        "item_str = get_item_str(query)\n",
        "# 得到item_str的MinHash\n",
        "minhash_query = get_minhash(item_str)\n",
        "\n",
        "# 查询forest中与m1相似的Top-K个邻居\n",
        "result = forest.query(minhash_query, 3)\n",
        "for i in range(len(result)):\n",
        "    print(result[i], minhash_query.jaccard(minhash_list[result[i]]), documents[result[i]].replace(' ', ''))\n",
        "print(\"Top 3 邻居\", result)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.914 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10 1.0 00:01:36，2019天猫双11总成交额超100亿元\n",
            "12 0.8125 00:12:49，2019天猫双11总成交额超500亿元\n",
            "14 0.9375 01:03:59，2019天猫双11总成交额超1000亿元\n",
            "Top 3 邻居 [10, 12, 14]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ90BnThjebM"
      },
      "source": [
        "# 对微博新闻进行相似句子Top-K查询\n",
        "from datasketch import MinHash, MinHashLSH, MinHashLSHForest\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import jieba.posseg as pseg\n",
        "import re"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGd1D22FjmdZ",
        "outputId": "b5cc34f5-b182-4199-8fcd-5fd0a48d906b"
      },
      "source": [
        "# 读取文件\n",
        "f = open('/content/gdrive/My Drive/LSH/weibos.txt', 'r', encoding='UTF-8')\n",
        "text = f.read()\n",
        "# 以标点符号作为分隔，去掉\\n换行符号\n",
        "sentences = re.split('[。！？# ……]', text.replace('\\n', ' '))\n",
        "\n",
        "# 最后一行如果为空，则删除\n",
        "if sentences[len(sentences)-1] == '':\n",
        "    sentences.pop()\n",
        "\n",
        "# 将item_text进行分词\n",
        "def get_item_str(item_text):\n",
        "    item_str = \"\" \n",
        "    item=(pseg.cut(item_text)) \n",
        "    for i in list(item):\n",
        "        #去掉停用词\n",
        "        if i.word not in list(stop):  \n",
        "            item_str += i.word\n",
        "            item_str += \" \"\n",
        "    return item_str\n",
        "# 对item_str创建MinHash\n",
        "def get_minhash(item_str):\n",
        "    temp = MinHash()\n",
        "    for d in item_str:\n",
        "        temp.update(d.encode('utf8'))\n",
        "    return temp\n",
        "\n",
        "# 设置停用词\n",
        "#stop = [line.strip() for line in open('stopwords.txt',encoding='UTF-8').readlines()]\n",
        "stop = []\n",
        "# 得到分词后的documents\n",
        "documents = []\n",
        "for item_text in sentences:\n",
        "    # 将item_text进行分词\n",
        "    item_str = get_item_str(item_text)\n",
        "    documents.append(item_str)\n",
        "\n",
        "# 创建LSH Forest及MinHash对象\n",
        "minhash_list = []\n",
        "forest = MinHashLSHForest()\n",
        "for i in range(len(documents)):\n",
        "    #得到train_documents[i]的MinHash\n",
        "    temp = get_minhash(documents[i])\n",
        "    minhash_list.append(temp)\n",
        "    forest.add(i, temp)\n",
        "# index所有key，以便可以进行检索\n",
        "forest.index()\n",
        "\n",
        "query = '中国足协：接受里皮辞职请求，将深刻反思'\n",
        "# 将item_text进行分词\n",
        "item_str = get_item_str(query)\n",
        "# 得到item_str的MinHash\n",
        "minhash_query = get_minhash(item_str)\n",
        "print('Query:',query)\n",
        "# 查询forest中与m1相似的Top-K个邻居\n",
        "result = forest.query(minhash_query, 3)\n",
        "for i in range(len(result)):\n",
        "    print(result[i], minhash_query.jaccard(minhash_list[result[i]]), documents[result[i]].replace(' ', ''))\n",
        "print(\"Top 3 邻居\", result)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: 中国足协：接受里皮辞职请求，将深刻反思\n",
            "10 0.3046875 据了解，无论中国足协态度如何，里皮其实在宣布请辞同时已经去意已决\n",
            "43 1.0 中国足协：接受里皮辞职请求，将深刻反思\n",
            "6 0.0703125 谁将成为新主帅，成为广大球迷关注的焦点\n",
            "Top 3 邻居 [10, 43, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}